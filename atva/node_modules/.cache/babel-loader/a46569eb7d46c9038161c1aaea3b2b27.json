{"ast":null,"code":"const natural = require(\"natural\");\n\nconst WordPos = require(\"wordpos\");\n\nconst WeightedGraph = require('./WeightedGraph').WeightedGraph;\n\nclass Preprocesser {\n  constructor() {\n    this.tokenizer = new natural.SentenceTokenizer();\n  } //This method takes in a paragraph and returns a list of the sentences in the paragraph.\n\n\n  paragraphToSentences(string_to_process) {\n    try {\n      let result = this.tokenizer.tokenize(string_to_process);\n      return result;\n    } catch (err) {\n      return Error(\"Cannot toeknize the given string.\");\n    }\n  } //Cleans the sentences by removing punctuation and lowercasing capital letters.\n\n\n  cleanSentences(list_to_clean) {\n    let sentence_map = new Map();\n    const regex = /[&\\/\\\\#,+()$~%.'\":*?<>{}]/g;\n\n    for (let i = 0; i < list_to_clean.length; i++) {\n      let original_sentence = list_to_clean[i];\n      list_to_clean[i] = list_to_clean[i].toLowerCase();\n      list_to_clean[i] = list_to_clean[i].replace(regex, \"\");\n      sentence_map.set(list_to_clean[i], original_sentence);\n    }\n\n    return [list_to_clean, sentence_map];\n  } //Takes in a list of sentences and returns a list of all of the words in the sentences.\n\n\n  tokenizeSentences(list_of_sentences) {\n    let new_array = new Array();\n    new_array = list_of_sentences;\n    let result_list = [];\n\n    for (let i = 0; i < new_array.length; i++) {\n      result_list = result_list.concat(new_array[i].split(\" \"));\n    }\n\n    return result_list;\n  } //Takes in a list of words and calculates the frequencies of the words.\n  //Returns a list. The first item is a map of word->frequency. The second is the max frequency.\n\n\n  getFrequencyAndMax(list_of_words) {\n    let frequency_map = new Map();\n    let max = 0;\n\n    for (let i = 0; i < list_of_words.length; i++) {\n      const word = list_of_words[i];\n\n      if (frequency_map.has(word)) {\n        const new_val = frequency_map.get(word) + 1;\n        frequency_map.set(word, new_val);\n\n        if (new_val > max) {\n          max = new_val;\n        }\n      } else {\n        frequency_map.set(word, 1);\n      }\n    }\n\n    return [frequency_map, max];\n  } //Converts a frequency map into a map with \"weights\".\n\n\n  getWeights(list_of_words) {\n    const frequencies_and_max = this.getFrequencyAndMax(list_of_words);\n    const frequencies_map = frequencies_and_max[0];\n    const max = frequencies_and_max[1];\n    frequencies_map.forEach((value, key, map) => {\n      map.set(key, value / max);\n    });\n    return frequencies_map;\n  }\n\n  sentenceWeights(clean_sentences, weighted_map) {\n    let weight_of_sentence = 0;\n    let sentence_weight_list = [];\n    let sentence = \"\";\n\n    for (let i = 0; i < clean_sentences.length; i++) {\n      sentence = clean_sentences[i];\n      let word_list = sentence.split(\" \");\n      weight_of_sentence = 0;\n\n      for (let j = 0; j < word_list.length; j++) {\n        weight_of_sentence += weighted_map.get(word_list[j]);\n      }\n\n      sentence_weight_list.push([weight_of_sentence / word_list.length, sentence]);\n    }\n\n    return sentence_weight_list;\n  } //Takes a list of sentences and returns a map of the each sentence to its nouns and adjectives\n\n\n  async nounsAndAdjectives(clean_sentences) {\n    let nouns_and_adjectives_map = new Map();\n    let wordpos = new WordPos();\n\n    try {\n      for (let i = 0; i < clean_sentences.length; i++) {\n        let adjectives = await wordpos.getAdjectives(clean_sentences[i]);\n        let nouns = await wordpos.getNouns(clean_sentences[i]);\n        nouns_and_adjectives_map.set(clean_sentences[i], nouns.concat(adjectives));\n      }\n\n      return await nouns_and_adjectives_map;\n    } catch (err) {\n      console.log(err);\n      return;\n    }\n  } //Used for the text rank summary. Takes two lists of words and gets the weight of the edge connecting the vertices.\n\n\n  getEdgeWeights(list1, list2) {\n    let weight = 0;\n    let intial = list1;\n    let other = list2;\n\n    if (list2.length >= list1.length) {\n      intial = list2;\n      other = list1;\n    }\n\n    for (let i = 0; i < intial.length; i++) {\n      if (other.includes(intial[i])) {\n        weight += 1;\n      }\n    }\n\n    return weight;\n  } //Creates the graph for the textrank algorithm.\n\n\n  createTextRankGraph(nouns_and_adjactive_map) {\n    let graph = new WeightedGraph();\n    let key_list = [];\n    let weight = 0;\n    nouns_and_adjactive_map.forEach((value, key, map) => {\n      key_list.push(key);\n    });\n\n    for (let i = 0; i < key_list.length; i++) {\n      for (let j = i + 1; j < key_list.length; j++) {\n        weight = this.getEdgeWeights(nouns_and_adjactive_map.get(key_list[i]), nouns_and_adjactive_map.get(key_list[j]));\n\n        if (weight > 0) {\n          graph.addEdge(key_list[i], key_list[j], weight);\n        }\n      }\n    }\n\n    return graph;\n  } //TextRank algorithm.\n\n\n  textRank(graph) {\n    let key_list = graph.getAllVertices();\n    let text_rank_map = new Map(); //random key to start with\n\n    if (key_list.length == 0) {\n      return text_rank_map;\n    }\n\n    let key = key_list[Math.floor(Math.random() * key_list.length)];\n    let vertex = graph.getVertex(key);\n    let probability_list = []; //random walk \n\n    for (let i = 0; i < 10000; i++) {\n      let full_weight = 0;\n      vertex.adjacent.forEach((value, key, map) => {\n        full_weight += value;\n      });\n      vertex.adjacent.forEach((value, key, map) => {\n        for (let x = 0; x < value; x++) {\n          probability_list.push(key);\n        }\n      });\n      let sentence = probability_list[Math.floor(Math.random() * probability_list.length)];\n\n      if (text_rank_map.has(sentence)) {\n        text_rank_map.set(sentence, text_rank_map.get(sentence) + 1);\n      } else {\n        text_rank_map.set(sentence, 1);\n      }\n\n      let last_vertex = vertex;\n      vertex = graph.getVertex(sentence);\n      probability_list = [];\n    }\n\n    return text_rank_map;\n  }\n\n}\n\nmodule.exports.Preprocesser = Preprocesser;","map":{"version":3,"sources":["/Users/tenzy/Documents/work/vata-hackathon-fe/atva/node_modules/node-summarizer/src/Preprocesser.js"],"names":["natural","require","WordPos","WeightedGraph","Preprocesser","constructor","tokenizer","SentenceTokenizer","paragraphToSentences","string_to_process","result","tokenize","err","Error","cleanSentences","list_to_clean","sentence_map","Map","regex","i","length","original_sentence","toLowerCase","replace","set","tokenizeSentences","list_of_sentences","new_array","Array","result_list","concat","split","getFrequencyAndMax","list_of_words","frequency_map","max","word","has","new_val","get","getWeights","frequencies_and_max","frequencies_map","forEach","value","key","map","sentenceWeights","clean_sentences","weighted_map","weight_of_sentence","sentence_weight_list","sentence","word_list","j","push","nounsAndAdjectives","nouns_and_adjectives_map","wordpos","adjectives","getAdjectives","nouns","getNouns","console","log","getEdgeWeights","list1","list2","weight","intial","other","includes","createTextRankGraph","nouns_and_adjactive_map","graph","key_list","addEdge","textRank","getAllVertices","text_rank_map","Math","floor","random","vertex","getVertex","probability_list","full_weight","adjacent","x","last_vertex","module","exports"],"mappings":"AAAA,MAAMA,OAAO,GAAGC,OAAO,CAAC,SAAD,CAAvB;;AACA,MAAMC,OAAO,GAAGD,OAAO,CAAC,SAAD,CAAvB;;AACA,MAAME,aAAa,GAAGF,OAAO,CAAC,iBAAD,CAAP,CAA2BE,aAAjD;;AAEA,MAAMC,YAAN,CAAkB;AACjBC,EAAAA,WAAW,GAAE;AACZ,SAAKC,SAAL,GAAiB,IAAIN,OAAO,CAACO,iBAAZ,EAAjB;AACA,GAHgB,CAKjB;;;AACAC,EAAAA,oBAAoB,CAACC,iBAAD,EAAmB;AACtC,QAAG;AACF,UAAIC,MAAM,GAAG,KAAKJ,SAAL,CAAeK,QAAf,CAAwBF,iBAAxB,CAAb;AACA,aAAOC,MAAP;AACA,KAHD,CAGC,OAAME,GAAN,EAAU;AACV,aAAOC,KAAK,CAAC,mCAAD,CAAZ;AACA;AACD,GAbgB,CAejB;;;AACAC,EAAAA,cAAc,CAACC,aAAD,EAAe;AAC5B,QAAIC,YAAY,GAAG,IAAIC,GAAJ,EAAnB;AACA,UAAMC,KAAK,GAAG,4BAAd;;AACA,SAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAACJ,aAAa,CAACK,MAAhC,EAAwCD,CAAC,EAAzC,EAA4C;AAC3C,UAAIE,iBAAiB,GAAGN,aAAa,CAACI,CAAD,CAArC;AACAJ,MAAAA,aAAa,CAACI,CAAD,CAAb,GAAmBJ,aAAa,CAACI,CAAD,CAAb,CAAiBG,WAAjB,EAAnB;AACAP,MAAAA,aAAa,CAACI,CAAD,CAAb,GAAmBJ,aAAa,CAACI,CAAD,CAAb,CAAiBI,OAAjB,CAAyBL,KAAzB,EAAgC,EAAhC,CAAnB;AACAF,MAAAA,YAAY,CAACQ,GAAb,CAAiBT,aAAa,CAACI,CAAD,CAA9B,EAAmCE,iBAAnC;AACA;;AACD,WAAO,CAACN,aAAD,EAAeC,YAAf,CAAP;AACA,GA1BgB,CA4BjB;;;AACAS,EAAAA,iBAAiB,CAACC,iBAAD,EAAmB;AACnC,QAAIC,SAAS,GAAG,IAAIC,KAAJ,EAAhB;AACAD,IAAAA,SAAS,GAAGD,iBAAZ;AACA,QAAIG,WAAW,GAAG,EAAlB;;AACA,SAAK,IAAIV,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAACQ,SAAS,CAACP,MAA5B,EAAoCD,CAAC,EAArC,EAAwC;AACvCU,MAAAA,WAAW,GAAGA,WAAW,CAACC,MAAZ,CAAmBH,SAAS,CAACR,CAAD,CAAT,CAAaY,KAAb,CAAmB,GAAnB,CAAnB,CAAd;AACA;;AACD,WAAOF,WAAP;AACA,GArCgB,CAuCjB;AACA;;;AACAG,EAAAA,kBAAkB,CAACC,aAAD,EAAe;AAChC,QAAIC,aAAa,GAAG,IAAIjB,GAAJ,EAApB;AACA,QAAIkB,GAAG,GAAG,CAAV;;AACA,SAAK,IAAIhB,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAACc,aAAa,CAACb,MAAhC,EAAwCD,CAAC,EAAzC,EAA4C;AAC3C,YAAMiB,IAAI,GAAGH,aAAa,CAACd,CAAD,CAA1B;;AACA,UAAIe,aAAa,CAACG,GAAd,CAAkBD,IAAlB,CAAJ,EAA4B;AAC3B,cAAME,OAAO,GAAGJ,aAAa,CAACK,GAAd,CAAkBH,IAAlB,IAAwB,CAAxC;AACAF,QAAAA,aAAa,CAACV,GAAd,CAAkBY,IAAlB,EAAwBE,OAAxB;;AACA,YAAIA,OAAO,GAACH,GAAZ,EAAgB;AACfA,UAAAA,GAAG,GAAGG,OAAN;AACA;AACD,OAND,MAMK;AACJJ,QAAAA,aAAa,CAACV,GAAd,CAAkBY,IAAlB,EAAwB,CAAxB;AACA;AACD;;AACD,WAAO,CAACF,aAAD,EAAgBC,GAAhB,CAAP;AACA,GAzDgB,CA2DjB;;;AACAK,EAAAA,UAAU,CAACP,aAAD,EAAe;AACxB,UAAMQ,mBAAmB,GAAG,KAAKT,kBAAL,CAAwBC,aAAxB,CAA5B;AACA,UAAMS,eAAe,GAAGD,mBAAmB,CAAC,CAAD,CAA3C;AACA,UAAMN,GAAG,GAAGM,mBAAmB,CAAC,CAAD,CAA/B;AACAC,IAAAA,eAAe,CAACC,OAAhB,CAAwB,CAACC,KAAD,EAAOC,GAAP,EAAWC,GAAX,KAAiB;AACxCA,MAAAA,GAAG,CAACtB,GAAJ,CAAQqB,GAAR,EAAaD,KAAK,GAACT,GAAnB;AACA,KAFD;AAGA,WAAOO,eAAP;AACA;;AAGDK,EAAAA,eAAe,CAACC,eAAD,EAAkBC,YAAlB,EAA+B;AAC7C,QAAIC,kBAAkB,GAAG,CAAzB;AACA,QAAIC,oBAAoB,GAAG,EAA3B;AACA,QAAIC,QAAQ,GAAG,EAAf;;AACA,SAAK,IAAIjC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAC6B,eAAe,CAAC5B,MAAlC,EAA0CD,CAAC,EAA3C,EAA8C;AAC7CiC,MAAAA,QAAQ,GAAGJ,eAAe,CAAC7B,CAAD,CAA1B;AACA,UAAIkC,SAAS,GAAGD,QAAQ,CAACrB,KAAT,CAAe,GAAf,CAAhB;AACAmB,MAAAA,kBAAkB,GAAG,CAArB;;AACA,WAAK,IAAII,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAACD,SAAS,CAACjC,MAA5B,EAAoCkC,CAAC,EAArC,EAAwC;AACvCJ,QAAAA,kBAAkB,IAAID,YAAY,CAACV,GAAb,CAAiBc,SAAS,CAACC,CAAD,CAA1B,CAAtB;AACA;;AACDH,MAAAA,oBAAoB,CAACI,IAArB,CAA0B,CAACL,kBAAkB,GAACG,SAAS,CAACjC,MAA9B,EAAsCgC,QAAtC,CAA1B;AACA;;AACD,WAAOD,oBAAP;AACA,GArFgB,CAuFjB;;;AACA,QAAMK,kBAAN,CAAyBR,eAAzB,EAAyC;AACxC,QAAIS,wBAAwB,GAAG,IAAIxC,GAAJ,EAA/B;AACA,QAAIyC,OAAO,GAAG,IAAIxD,OAAJ,EAAd;;AACA,QAAG;AACF,WAAK,IAAIiB,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAC6B,eAAe,CAAC5B,MAAlC,EAA0CD,CAAC,EAA3C,EAA8C;AAC7C,YAAIwC,UAAU,GAAG,MAAMD,OAAO,CAACE,aAAR,CAAsBZ,eAAe,CAAC7B,CAAD,CAArC,CAAvB;AACA,YAAI0C,KAAK,GAAG,MAAMH,OAAO,CAACI,QAAR,CAAiBd,eAAe,CAAC7B,CAAD,CAAhC,CAAlB;AACAsC,QAAAA,wBAAwB,CAACjC,GAAzB,CAA6BwB,eAAe,CAAC7B,CAAD,CAA5C,EAAgD0C,KAAK,CAAC/B,MAAN,CAAa6B,UAAb,CAAhD;AACA;;AAED,aAAO,MAAMF,wBAAb;AACA,KARD,CAQC,OAAM7C,GAAN,EAAU;AACVmD,MAAAA,OAAO,CAACC,GAAR,CAAYpD,GAAZ;AACA;AACA;AACD,GAvGgB,CAyGjB;;;AACAqD,EAAAA,cAAc,CAACC,KAAD,EAAQC,KAAR,EAAc;AAC3B,QAAIC,MAAM,GAAG,CAAb;AACA,QAAIC,MAAM,GAAGH,KAAb;AACA,QAAII,KAAK,GAAGH,KAAZ;;AACA,QAAIA,KAAK,CAAC/C,MAAN,IAAgB8C,KAAK,CAAC9C,MAA1B,EAAiC;AAChCiD,MAAAA,MAAM,GAAGF,KAAT;AACAG,MAAAA,KAAK,GAAGJ,KAAR;AACA;;AACD,SAAI,IAAI/C,CAAC,GAAC,CAAV,EAAaA,CAAC,GAACkD,MAAM,CAACjD,MAAtB,EAA8BD,CAAC,EAA/B,EAAkC;AACjC,UAAGmD,KAAK,CAACC,QAAN,CAAeF,MAAM,CAAClD,CAAD,CAArB,CAAH,EAA6B;AAC5BiD,QAAAA,MAAM,IAAE,CAAR;AACA;AACD;;AAED,WAAOA,MAAP;AACA,GAzHgB,CA2HjB;;;AACAI,EAAAA,mBAAmB,CAACC,uBAAD,EAAyB;AAC3C,QAAIC,KAAK,GAAG,IAAIvE,aAAJ,EAAZ;AACA,QAAIwE,QAAQ,GAAG,EAAf;AACA,QAAIP,MAAM,GAAG,CAAb;AACAK,IAAAA,uBAAuB,CAAC9B,OAAxB,CAAgC,CAACC,KAAD,EAAOC,GAAP,EAAWC,GAAX,KAAiB;AAChD6B,MAAAA,QAAQ,CAACpB,IAAT,CAAcV,GAAd;AACA,KAFD;;AAGA,SAAI,IAAI1B,CAAC,GAAC,CAAV,EAAaA,CAAC,GAACwD,QAAQ,CAACvD,MAAxB,EAAgCD,CAAC,EAAjC,EAAoC;AACnC,WAAI,IAAImC,CAAC,GAACnC,CAAC,GAAC,CAAZ,EAAemC,CAAC,GAACqB,QAAQ,CAACvD,MAA1B,EAAkCkC,CAAC,EAAnC,EAAsC;AACrCc,QAAAA,MAAM,GAAG,KAAKH,cAAL,CAAoBQ,uBAAuB,CAAClC,GAAxB,CAA4BoC,QAAQ,CAACxD,CAAD,CAApC,CAApB,EAA8DsD,uBAAuB,CAAClC,GAAxB,CAA4BoC,QAAQ,CAACrB,CAAD,CAApC,CAA9D,CAAT;;AACA,YAAGc,MAAM,GAAC,CAAV,EAAY;AACXM,UAAAA,KAAK,CAACE,OAAN,CAAcD,QAAQ,CAACxD,CAAD,CAAtB,EAA2BwD,QAAQ,CAACrB,CAAD,CAAnC,EAAwCc,MAAxC;AACA;AACD;AAED;;AACD,WAAOM,KAAP;AACA,GA7IgB,CA+IjB;;;AACAG,EAAAA,QAAQ,CAACH,KAAD,EAAO;AACd,QAAIC,QAAQ,GAAGD,KAAK,CAACI,cAAN,EAAf;AACA,QAAIC,aAAa,GAAG,IAAI9D,GAAJ,EAApB,CAFc,CAId;;AACA,QAAI0D,QAAQ,CAACvD,MAAT,IAAmB,CAAvB,EAAyB;AACxB,aAAO2D,aAAP;AACA;;AACD,QAAIlC,GAAG,GAAG8B,QAAQ,CAACK,IAAI,CAACC,KAAL,CAAWD,IAAI,CAACE,MAAL,KAAcP,QAAQ,CAACvD,MAAlC,CAAD,CAAlB;AACA,QAAI+D,MAAM,GAAGT,KAAK,CAACU,SAAN,CAAgBvC,GAAhB,CAAb;AACA,QAAIwC,gBAAgB,GAAG,EAAvB,CAVc,CAWd;;AACA,SAAK,IAAIlE,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG,KAApB,EAA2BA,CAAC,EAA5B,EAAgC;AAC/B,UAAImE,WAAW,GAAG,CAAlB;AAEAH,MAAAA,MAAM,CAACI,QAAP,CAAgB5C,OAAhB,CAAwB,CAACC,KAAD,EAAQC,GAAR,EAAaC,GAAb,KAAmB;AAC1CwC,QAAAA,WAAW,IAAE1C,KAAb;AACA,OAFD;AAIAuC,MAAAA,MAAM,CAACI,QAAP,CAAgB5C,OAAhB,CAAwB,CAACC,KAAD,EAAQC,GAAR,EAAaC,GAAb,KAAmB;AAC1C,aAAI,IAAI0C,CAAC,GAAG,CAAZ,EAAeA,CAAC,GAAC5C,KAAjB,EAAwB4C,CAAC,EAAzB,EAA4B;AAC3BH,UAAAA,gBAAgB,CAAC9B,IAAjB,CAAsBV,GAAtB;AACA;AACD,OAJD;AAOA,UAAIO,QAAQ,GAAGiC,gBAAgB,CAACL,IAAI,CAACC,KAAL,CAAWD,IAAI,CAACE,MAAL,KAAcG,gBAAgB,CAACjE,MAA1C,CAAD,CAA/B;;AACA,UAAG2D,aAAa,CAAC1C,GAAd,CAAkBe,QAAlB,CAAH,EAA+B;AAC9B2B,QAAAA,aAAa,CAACvD,GAAd,CAAkB4B,QAAlB,EAA4B2B,aAAa,CAACxC,GAAd,CAAkBa,QAAlB,IAA4B,CAAxD;AACA,OAFD,MAEK;AACJ2B,QAAAA,aAAa,CAACvD,GAAd,CAAkB4B,QAAlB,EAA4B,CAA5B;AACA;;AACD,UAAIqC,WAAW,GAAGN,MAAlB;AACAA,MAAAA,MAAM,GAAGT,KAAK,CAACU,SAAN,CAAgBhC,QAAhB,CAAT;AACAiC,MAAAA,gBAAgB,GAAG,EAAnB;AACA;;AACD,WAAON,aAAP;AAEA;;AAtLgB;;AA4LlBW,MAAM,CAACC,OAAP,CAAevF,YAAf,GAA8BA,YAA9B","sourcesContent":["const natural = require(\"natural\");\nconst WordPos = require(\"wordpos\");\nconst WeightedGraph = require('./WeightedGraph').WeightedGraph;\n\nclass Preprocesser{\n\tconstructor(){\n\t\tthis.tokenizer = new natural.SentenceTokenizer(); \n\t}\n\n\t//This method takes in a paragraph and returns a list of the sentences in the paragraph.\n\tparagraphToSentences(string_to_process){\n\t\ttry{\n\t\t\tlet result = this.tokenizer.tokenize(string_to_process);\n\t\t\treturn result;\n\t\t}catch(err){\n\t\t\treturn Error(\"Cannot toeknize the given string.\");\n\t\t}\n\t}\n\n\t//Cleans the sentences by removing punctuation and lowercasing capital letters.\n\tcleanSentences(list_to_clean){\n\t\tlet sentence_map = new Map();\n\t\tconst regex = /[&\\/\\\\#,+()$~%.'\":*?<>{}]/g;\n\t\tfor (let i = 0; i<list_to_clean.length; i++){\n\t\t\tlet original_sentence = list_to_clean[i];\n\t\t\tlist_to_clean[i] = list_to_clean[i].toLowerCase();\n\t\t\tlist_to_clean[i] = list_to_clean[i].replace(regex, \"\");\n\t\t\tsentence_map.set(list_to_clean[i], original_sentence);\n\t\t}\n\t\treturn [list_to_clean,sentence_map];\n\t}\n\n\t//Takes in a list of sentences and returns a list of all of the words in the sentences.\n\ttokenizeSentences(list_of_sentences){\n\t\tlet new_array = new Array();\n\t\tnew_array = list_of_sentences\n\t\tlet result_list = [];\n\t\tfor (let i = 0; i<new_array.length; i++){\n\t\t\tresult_list = result_list.concat(new_array[i].split(\" \"));\n\t\t}\n\t\treturn result_list;\n\t}\n\n\t//Takes in a list of words and calculates the frequencies of the words.\n\t//Returns a list. The first item is a map of word->frequency. The second is the max frequency.\n\tgetFrequencyAndMax(list_of_words){\n\t\tlet frequency_map = new Map();\n\t\tlet max = 0\n\t\tfor (let i = 0; i<list_of_words.length; i++){\n\t\t\tconst word = list_of_words[i];\n\t\t\tif (frequency_map.has(word)){\n\t\t\t\tconst new_val = frequency_map.get(word)+1;\n\t\t\t\tfrequency_map.set(word, new_val);\n\t\t\t\tif (new_val>max){\n\t\t\t\t\tmax = new_val;\n\t\t\t\t}\n\t\t\t}else{\n\t\t\t\tfrequency_map.set(word, 1);\n\t\t\t}\n\t\t}\n\t\treturn [frequency_map, max];\n\t}\n\t\n\t//Converts a frequency map into a map with \"weights\".\n\tgetWeights(list_of_words){\n\t\tconst frequencies_and_max = this.getFrequencyAndMax(list_of_words);\n\t\tconst frequencies_map = frequencies_and_max[0];\n\t\tconst max = frequencies_and_max[1];\n\t\tfrequencies_map.forEach((value,key,map)=>{\n\t\t\tmap.set(key, value/max);\n\t\t});\n\t\treturn frequencies_map;\n\t}\n\n\n\tsentenceWeights(clean_sentences, weighted_map){\n\t\tlet weight_of_sentence = 0;\n\t\tlet sentence_weight_list = [];\n\t\tlet sentence = \"\";\n\t\tfor (let i = 0; i<clean_sentences.length; i++){\n\t\t\tsentence = clean_sentences[i];\n\t\t\tlet word_list = sentence.split(\" \");\n\t\t\tweight_of_sentence = 0;\n\t\t\tfor (let j = 0; j<word_list.length; j++){\n\t\t\t\tweight_of_sentence += weighted_map.get(word_list[j]);\n\t\t\t}\n\t\t\tsentence_weight_list.push([weight_of_sentence/word_list.length, sentence]);\n\t\t}\n\t\treturn sentence_weight_list;\n\t}\n\n\t//Takes a list of sentences and returns a map of the each sentence to its nouns and adjectives\n\tasync nounsAndAdjectives(clean_sentences){\n\t\tlet nouns_and_adjectives_map = new Map();\n\t\tlet wordpos = new WordPos();\n\t\ttry{\n\t\t\tfor (let i = 0; i<clean_sentences.length; i++){\n\t\t\t\tlet adjectives = await wordpos.getAdjectives(clean_sentences[i]);\n\t\t\t\tlet nouns = await wordpos.getNouns(clean_sentences[i]);\n\t\t\t\tnouns_and_adjectives_map.set(clean_sentences[i],nouns.concat(adjectives));\n\t\t\t}\n\n\t\t\treturn await nouns_and_adjectives_map;\n\t\t}catch(err){\n\t\t\tconsole.log(err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t//Used for the text rank summary. Takes two lists of words and gets the weight of the edge connecting the vertices.\n\tgetEdgeWeights(list1, list2){\n\t\tlet weight = 0;\n\t\tlet intial = list1\n\t\tlet other = list2\n\t\tif (list2.length >= list1.length){\n\t\t\tintial = list2\n\t\t\tother = list1\n\t\t}\n\t\tfor(let i=0; i<intial.length; i++){\n\t\t\tif(other.includes(intial[i])){\n\t\t\t\tweight+=1;\n\t\t\t}\n\t\t}\n\n\t\treturn weight\n\t}\n\n\t//Creates the graph for the textrank algorithm.\n\tcreateTextRankGraph(nouns_and_adjactive_map){\n\t\tlet graph = new WeightedGraph();\n\t\tlet key_list = [];\n\t\tlet weight = 0\n\t\tnouns_and_adjactive_map.forEach((value,key,map)=>{\n\t\t\tkey_list.push(key);\n\t\t})\n\t\tfor(let i=0; i<key_list.length; i++){\n\t\t\tfor(let j=i+1; j<key_list.length; j++){\n\t\t\t\tweight = this.getEdgeWeights(nouns_and_adjactive_map.get(key_list[i]), nouns_and_adjactive_map.get(key_list[j]));\n\t\t\t\tif(weight>0){\n\t\t\t\t\tgraph.addEdge(key_list[i], key_list[j], weight);\n\t\t\t\t}\n\t\t\t}\n\n\t\t}\n\t\treturn graph;\n\t}\n\n\t//TextRank algorithm.\n\ttextRank(graph){\n\t\tlet key_list = graph.getAllVertices();\n\t\tlet text_rank_map = new Map();\n\t\t\n\t\t//random key to start with\n\t\tif (key_list.length == 0){\n\t\t\treturn text_rank_map;\n\t\t}\n\t\tlet key = key_list[Math.floor(Math.random()*key_list.length)];\n\t\tlet vertex = graph.getVertex(key);\n\t\tlet probability_list = [];\n\t\t//random walk \n\t\tfor (let i = 0; i < 10000; i++) {\n\t\t\tlet full_weight = 0\n\t\t\n\t\t\tvertex.adjacent.forEach((value, key, map)=>{\n\t\t\t\tfull_weight+=value;\n\t\t\t})\n\t\t\n\t\t\tvertex.adjacent.forEach((value, key, map)=>{\n\t\t\t\tfor(let x = 0; x<value; x++){\n\t\t\t\t\tprobability_list.push(key);\n\t\t\t\t}\n\t\t\t})\n\t\t\n\n\t\t\tlet sentence = probability_list[Math.floor(Math.random()*probability_list.length)];\n\t\t\tif(text_rank_map.has(sentence)){\n\t\t\t\ttext_rank_map.set(sentence, text_rank_map.get(sentence)+1)\n\t\t\t}else{\n\t\t\t\ttext_rank_map.set(sentence, 1);\n\t\t\t}\n\t\t\tlet last_vertex = vertex;\n\t\t\tvertex = graph.getVertex(sentence);\n\t\t\tprobability_list = [];\n\t\t}\n\t\treturn text_rank_map;\n\t\t\n\t}\n\n\n}\n\n\nmodule.exports.Preprocesser = Preprocesser"]},"metadata":{},"sourceType":"script"}