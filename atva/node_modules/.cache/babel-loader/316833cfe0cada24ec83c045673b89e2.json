{"ast":null,"code":"var _regeneratorRuntime = require(\"/Users/tenzy/Documents/work/vata-hackathon-fe/atva/node_modules/@babel/runtime/regenerator\");\n\nvar _classCallCheck = require(\"/Users/tenzy/Documents/work/vata-hackathon-fe/atva/node_modules/@babel/runtime/helpers/classCallCheck\");\n\nvar _createClass = require(\"/Users/tenzy/Documents/work/vata-hackathon-fe/atva/node_modules/@babel/runtime/helpers/createClass\");\n\nvar natural = require(\"natural\");\n\nvar WordPos = require(\"wordpos\");\n\nvar WeightedGraph = require('./WeightedGraph').WeightedGraph;\n\nvar Preprocesser =\n/*#__PURE__*/\nfunction () {\n  \"use strict\";\n\n  function Preprocesser() {\n    _classCallCheck(this, Preprocesser);\n\n    this.tokenizer = new natural.SentenceTokenizer();\n  } //This method takes in a paragraph and returns a list of the sentences in the paragraph.\n\n\n  _createClass(Preprocesser, [{\n    key: \"paragraphToSentences\",\n    value: function paragraphToSentences(string_to_process) {\n      try {\n        var result = this.tokenizer.tokenize(string_to_process);\n        return result;\n      } catch (err) {\n        return Error(\"Cannot toeknize the given string.\");\n      }\n    } //Cleans the sentences by removing punctuation and lowercasing capital letters.\n\n  }, {\n    key: \"cleanSentences\",\n    value: function cleanSentences(list_to_clean) {\n      var sentence_map = new Map();\n      var regex = /[&\\/\\\\#,+()$~%.'\":*?<>{}]/g;\n\n      for (var i = 0; i < list_to_clean.length; i++) {\n        var original_sentence = list_to_clean[i];\n        list_to_clean[i] = list_to_clean[i].toLowerCase();\n        list_to_clean[i] = list_to_clean[i].replace(regex, \"\");\n        sentence_map.set(list_to_clean[i], original_sentence);\n      }\n\n      return [list_to_clean, sentence_map];\n    } //Takes in a list of sentences and returns a list of all of the words in the sentences.\n\n  }, {\n    key: \"tokenizeSentences\",\n    value: function tokenizeSentences(list_of_sentences) {\n      var new_array = new Array();\n      new_array = list_of_sentences;\n      var result_list = [];\n\n      for (var i = 0; i < new_array.length; i++) {\n        result_list = result_list.concat(new_array[i].split(\" \"));\n      }\n\n      return result_list;\n    } //Takes in a list of words and calculates the frequencies of the words.\n    //Returns a list. The first item is a map of word->frequency. The second is the max frequency.\n\n  }, {\n    key: \"getFrequencyAndMax\",\n    value: function getFrequencyAndMax(list_of_words) {\n      var frequency_map = new Map();\n      var max = 0;\n\n      for (var i = 0; i < list_of_words.length; i++) {\n        var word = list_of_words[i];\n\n        if (frequency_map.has(word)) {\n          var new_val = frequency_map.get(word) + 1;\n          frequency_map.set(word, new_val);\n\n          if (new_val > max) {\n            max = new_val;\n          }\n        } else {\n          frequency_map.set(word, 1);\n        }\n      }\n\n      return [frequency_map, max];\n    } //Converts a frequency map into a map with \"weights\".\n\n  }, {\n    key: \"getWeights\",\n    value: function getWeights(list_of_words) {\n      var frequencies_and_max = this.getFrequencyAndMax(list_of_words);\n      var frequencies_map = frequencies_and_max[0];\n      var max = frequencies_and_max[1];\n      frequencies_map.forEach(function (value, key, map) {\n        map.set(key, value / max);\n      });\n      return frequencies_map;\n    }\n  }, {\n    key: \"sentenceWeights\",\n    value: function sentenceWeights(clean_sentences, weighted_map) {\n      var weight_of_sentence = 0;\n      var sentence_weight_list = [];\n      var sentence = \"\";\n\n      for (var i = 0; i < clean_sentences.length; i++) {\n        sentence = clean_sentences[i];\n        var word_list = sentence.split(\" \");\n        weight_of_sentence = 0;\n\n        for (var j = 0; j < word_list.length; j++) {\n          weight_of_sentence += weighted_map.get(word_list[j]);\n        }\n\n        sentence_weight_list.push([weight_of_sentence / word_list.length, sentence]);\n      }\n\n      return sentence_weight_list;\n    } //Takes a list of sentences and returns a map of the each sentence to its nouns and adjectives\n\n  }, {\n    key: \"nounsAndAdjectives\",\n    value: function nounsAndAdjectives(clean_sentences) {\n      var nouns_and_adjectives_map, wordpos, i, adjectives, nouns;\n      return _regeneratorRuntime.async(function nounsAndAdjectives$(_context) {\n        while (1) {\n          switch (_context.prev = _context.next) {\n            case 0:\n              nouns_and_adjectives_map = new Map();\n              wordpos = new WordPos();\n              _context.prev = 2;\n              i = 0;\n\n            case 4:\n              if (!(i < clean_sentences.length)) {\n                _context.next = 15;\n                break;\n              }\n\n              _context.next = 7;\n              return _regeneratorRuntime.awrap(wordpos.getAdjectives(clean_sentences[i]));\n\n            case 7:\n              adjectives = _context.sent;\n              _context.next = 10;\n              return _regeneratorRuntime.awrap(wordpos.getNouns(clean_sentences[i]));\n\n            case 10:\n              nouns = _context.sent;\n              nouns_and_adjectives_map.set(clean_sentences[i], nouns.concat(adjectives));\n\n            case 12:\n              i++;\n              _context.next = 4;\n              break;\n\n            case 15:\n              _context.next = 17;\n              return _regeneratorRuntime.awrap(nouns_and_adjectives_map);\n\n            case 17:\n              return _context.abrupt(\"return\", _context.sent);\n\n            case 20:\n              _context.prev = 20;\n              _context.t0 = _context[\"catch\"](2);\n              console.log(_context.t0);\n              return _context.abrupt(\"return\");\n\n            case 24:\n            case \"end\":\n              return _context.stop();\n          }\n        }\n      }, null, null, [[2, 20]]);\n    } //Used for the text rank summary. Takes two lists of words and gets the weight of the edge connecting the vertices.\n\n  }, {\n    key: \"getEdgeWeights\",\n    value: function getEdgeWeights(list1, list2) {\n      var weight = 0;\n      var intial = list1;\n      var other = list2;\n\n      if (list2.length >= list1.length) {\n        intial = list2;\n        other = list1;\n      }\n\n      for (var i = 0; i < intial.length; i++) {\n        if (other.includes(intial[i])) {\n          weight += 1;\n        }\n      }\n\n      return weight;\n    } //Creates the graph for the textrank algorithm.\n\n  }, {\n    key: \"createTextRankGraph\",\n    value: function createTextRankGraph(nouns_and_adjactive_map) {\n      var graph = new WeightedGraph();\n      var key_list = [];\n      var weight = 0;\n      nouns_and_adjactive_map.forEach(function (value, key, map) {\n        key_list.push(key);\n      });\n\n      for (var i = 0; i < key_list.length; i++) {\n        for (var j = i + 1; j < key_list.length; j++) {\n          weight = this.getEdgeWeights(nouns_and_adjactive_map.get(key_list[i]), nouns_and_adjactive_map.get(key_list[j]));\n\n          if (weight > 0) {\n            graph.addEdge(key_list[i], key_list[j], weight);\n          }\n        }\n      }\n\n      return graph;\n    } //TextRank algorithm.\n\n  }, {\n    key: \"textRank\",\n    value: function textRank(graph) {\n      var key_list = graph.getAllVertices();\n      var text_rank_map = new Map(); //random key to start with\n\n      if (key_list.length == 0) {\n        return text_rank_map;\n      }\n\n      var key = key_list[Math.floor(Math.random() * key_list.length)];\n      var vertex = graph.getVertex(key);\n      var probability_list = []; //random walk \n\n      for (var i = 0; i < 10000; i++) {\n        var full_weight = 0;\n        vertex.adjacent.forEach(function (value, key, map) {\n          full_weight += value;\n        });\n        vertex.adjacent.forEach(function (value, key, map) {\n          for (var x = 0; x < value; x++) {\n            probability_list.push(key);\n          }\n        });\n        var sentence = probability_list[Math.floor(Math.random() * probability_list.length)];\n\n        if (text_rank_map.has(sentence)) {\n          text_rank_map.set(sentence, text_rank_map.get(sentence) + 1);\n        } else {\n          text_rank_map.set(sentence, 1);\n        }\n\n        var last_vertex = vertex;\n        vertex = graph.getVertex(sentence);\n        probability_list = [];\n      }\n\n      return text_rank_map;\n    }\n  }]);\n\n  return Preprocesser;\n}();\n\nmodule.exports.Preprocesser = Preprocesser;","map":{"version":3,"sources":["/Users/tenzy/Documents/work/vata-hackathon-fe/atva/node_modules/node-summarizer/src/Preprocesser.js"],"names":["natural","require","WordPos","WeightedGraph","Preprocesser","tokenizer","SentenceTokenizer","string_to_process","result","tokenize","err","Error","list_to_clean","sentence_map","Map","regex","i","length","original_sentence","toLowerCase","replace","set","list_of_sentences","new_array","Array","result_list","concat","split","list_of_words","frequency_map","max","word","has","new_val","get","frequencies_and_max","getFrequencyAndMax","frequencies_map","forEach","value","key","map","clean_sentences","weighted_map","weight_of_sentence","sentence_weight_list","sentence","word_list","j","push","nouns_and_adjectives_map","wordpos","getAdjectives","adjectives","getNouns","nouns","console","log","list1","list2","weight","intial","other","includes","nouns_and_adjactive_map","graph","key_list","getEdgeWeights","addEdge","getAllVertices","text_rank_map","Math","floor","random","vertex","getVertex","probability_list","full_weight","adjacent","x","last_vertex","module","exports"],"mappings":";;;;;;AAAA,IAAMA,OAAO,GAAGC,OAAO,CAAC,SAAD,CAAvB;;AACA,IAAMC,OAAO,GAAGD,OAAO,CAAC,SAAD,CAAvB;;AACA,IAAME,aAAa,GAAGF,OAAO,CAAC,iBAAD,CAAP,CAA2BE,aAAjD;;IAEMC,Y;;;;;AACL,0BAAa;AAAA;;AACZ,SAAKC,SAAL,GAAiB,IAAIL,OAAO,CAACM,iBAAZ,EAAjB;AACA,G,CAED;;;;;yCACqBC,iB,EAAkB;AACtC,UAAG;AACF,YAAIC,MAAM,GAAG,KAAKH,SAAL,CAAeI,QAAf,CAAwBF,iBAAxB,CAAb;AACA,eAAOC,MAAP;AACA,OAHD,CAGC,OAAME,GAAN,EAAU;AACV,eAAOC,KAAK,CAAC,mCAAD,CAAZ;AACA;AACD,K,CAED;;;;mCACeC,a,EAAc;AAC5B,UAAIC,YAAY,GAAG,IAAIC,GAAJ,EAAnB;AACA,UAAMC,KAAK,GAAG,4BAAd;;AACA,WAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAACJ,aAAa,CAACK,MAAhC,EAAwCD,CAAC,EAAzC,EAA4C;AAC3C,YAAIE,iBAAiB,GAAGN,aAAa,CAACI,CAAD,CAArC;AACAJ,QAAAA,aAAa,CAACI,CAAD,CAAb,GAAmBJ,aAAa,CAACI,CAAD,CAAb,CAAiBG,WAAjB,EAAnB;AACAP,QAAAA,aAAa,CAACI,CAAD,CAAb,GAAmBJ,aAAa,CAACI,CAAD,CAAb,CAAiBI,OAAjB,CAAyBL,KAAzB,EAAgC,EAAhC,CAAnB;AACAF,QAAAA,YAAY,CAACQ,GAAb,CAAiBT,aAAa,CAACI,CAAD,CAA9B,EAAmCE,iBAAnC;AACA;;AACD,aAAO,CAACN,aAAD,EAAeC,YAAf,CAAP;AACA,K,CAED;;;;sCACkBS,iB,EAAkB;AACnC,UAAIC,SAAS,GAAG,IAAIC,KAAJ,EAAhB;AACAD,MAAAA,SAAS,GAAGD,iBAAZ;AACA,UAAIG,WAAW,GAAG,EAAlB;;AACA,WAAK,IAAIT,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAACO,SAAS,CAACN,MAA5B,EAAoCD,CAAC,EAArC,EAAwC;AACvCS,QAAAA,WAAW,GAAGA,WAAW,CAACC,MAAZ,CAAmBH,SAAS,CAACP,CAAD,CAAT,CAAaW,KAAb,CAAmB,GAAnB,CAAnB,CAAd;AACA;;AACD,aAAOF,WAAP;AACA,K,CAED;AACA;;;;uCACmBG,a,EAAc;AAChC,UAAIC,aAAa,GAAG,IAAIf,GAAJ,EAApB;AACA,UAAIgB,GAAG,GAAG,CAAV;;AACA,WAAK,IAAId,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAACY,aAAa,CAACX,MAAhC,EAAwCD,CAAC,EAAzC,EAA4C;AAC3C,YAAMe,IAAI,GAAGH,aAAa,CAACZ,CAAD,CAA1B;;AACA,YAAIa,aAAa,CAACG,GAAd,CAAkBD,IAAlB,CAAJ,EAA4B;AAC3B,cAAME,OAAO,GAAGJ,aAAa,CAACK,GAAd,CAAkBH,IAAlB,IAAwB,CAAxC;AACAF,UAAAA,aAAa,CAACR,GAAd,CAAkBU,IAAlB,EAAwBE,OAAxB;;AACA,cAAIA,OAAO,GAACH,GAAZ,EAAgB;AACfA,YAAAA,GAAG,GAAGG,OAAN;AACA;AACD,SAND,MAMK;AACJJ,UAAAA,aAAa,CAACR,GAAd,CAAkBU,IAAlB,EAAwB,CAAxB;AACA;AACD;;AACD,aAAO,CAACF,aAAD,EAAgBC,GAAhB,CAAP;AACA,K,CAED;;;;+BACWF,a,EAAc;AACxB,UAAMO,mBAAmB,GAAG,KAAKC,kBAAL,CAAwBR,aAAxB,CAA5B;AACA,UAAMS,eAAe,GAAGF,mBAAmB,CAAC,CAAD,CAA3C;AACA,UAAML,GAAG,GAAGK,mBAAmB,CAAC,CAAD,CAA/B;AACAE,MAAAA,eAAe,CAACC,OAAhB,CAAwB,UAACC,KAAD,EAAOC,GAAP,EAAWC,GAAX,EAAiB;AACxCA,QAAAA,GAAG,CAACpB,GAAJ,CAAQmB,GAAR,EAAaD,KAAK,GAACT,GAAnB;AACA,OAFD;AAGA,aAAOO,eAAP;AACA;;;oCAGeK,e,EAAiBC,Y,EAAa;AAC7C,UAAIC,kBAAkB,GAAG,CAAzB;AACA,UAAIC,oBAAoB,GAAG,EAA3B;AACA,UAAIC,QAAQ,GAAG,EAAf;;AACA,WAAK,IAAI9B,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAC0B,eAAe,CAACzB,MAAlC,EAA0CD,CAAC,EAA3C,EAA8C;AAC7C8B,QAAAA,QAAQ,GAAGJ,eAAe,CAAC1B,CAAD,CAA1B;AACA,YAAI+B,SAAS,GAAGD,QAAQ,CAACnB,KAAT,CAAe,GAAf,CAAhB;AACAiB,QAAAA,kBAAkB,GAAG,CAArB;;AACA,aAAK,IAAII,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAACD,SAAS,CAAC9B,MAA5B,EAAoC+B,CAAC,EAArC,EAAwC;AACvCJ,UAAAA,kBAAkB,IAAID,YAAY,CAACT,GAAb,CAAiBa,SAAS,CAACC,CAAD,CAA1B,CAAtB;AACA;;AACDH,QAAAA,oBAAoB,CAACI,IAArB,CAA0B,CAACL,kBAAkB,GAACG,SAAS,CAAC9B,MAA9B,EAAsC6B,QAAtC,CAA1B;AACA;;AACD,aAAOD,oBAAP;AACA,K,CAED;;;;uCACyBH,e;;;;;;AACpBQ,cAAAA,wB,GAA2B,IAAIpC,GAAJ,E;AAC3BqC,cAAAA,O,GAAU,IAAIjD,OAAJ,E;;AAEJc,cAAAA,C,GAAI,C;;;oBAAGA,CAAC,GAAC0B,eAAe,CAACzB,M;;;;;;+CACVkC,OAAO,CAACC,aAAR,CAAsBV,eAAe,CAAC1B,CAAD,CAArC,C;;;AAAnBqC,cAAAA,U;;+CACcF,OAAO,CAACG,QAAR,CAAiBZ,eAAe,CAAC1B,CAAD,CAAhC,C;;;AAAduC,cAAAA,K;AACJL,cAAAA,wBAAwB,CAAC7B,GAAzB,CAA6BqB,eAAe,CAAC1B,CAAD,CAA5C,EAAgDuC,KAAK,CAAC7B,MAAN,CAAa2B,UAAb,CAAhD;;;AAHyCrC,cAAAA,CAAC,E;;;;;;+CAM9BkC,wB;;;;;;;;AAEbM,cAAAA,OAAO,CAACC,GAAR;;;;;;;;;MAKF;;;;mCACeC,K,EAAOC,K,EAAM;AAC3B,UAAIC,MAAM,GAAG,CAAb;AACA,UAAIC,MAAM,GAAGH,KAAb;AACA,UAAII,KAAK,GAAGH,KAAZ;;AACA,UAAIA,KAAK,CAAC1C,MAAN,IAAgByC,KAAK,CAACzC,MAA1B,EAAiC;AAChC4C,QAAAA,MAAM,GAAGF,KAAT;AACAG,QAAAA,KAAK,GAAGJ,KAAR;AACA;;AACD,WAAI,IAAI1C,CAAC,GAAC,CAAV,EAAaA,CAAC,GAAC6C,MAAM,CAAC5C,MAAtB,EAA8BD,CAAC,EAA/B,EAAkC;AACjC,YAAG8C,KAAK,CAACC,QAAN,CAAeF,MAAM,CAAC7C,CAAD,CAArB,CAAH,EAA6B;AAC5B4C,UAAAA,MAAM,IAAE,CAAR;AACA;AACD;;AAED,aAAOA,MAAP;AACA,K,CAED;;;;wCACoBI,uB,EAAwB;AAC3C,UAAIC,KAAK,GAAG,IAAI9D,aAAJ,EAAZ;AACA,UAAI+D,QAAQ,GAAG,EAAf;AACA,UAAIN,MAAM,GAAG,CAAb;AACAI,MAAAA,uBAAuB,CAAC1B,OAAxB,CAAgC,UAACC,KAAD,EAAOC,GAAP,EAAWC,GAAX,EAAiB;AAChDyB,QAAAA,QAAQ,CAACjB,IAAT,CAAcT,GAAd;AACA,OAFD;;AAGA,WAAI,IAAIxB,CAAC,GAAC,CAAV,EAAaA,CAAC,GAACkD,QAAQ,CAACjD,MAAxB,EAAgCD,CAAC,EAAjC,EAAoC;AACnC,aAAI,IAAIgC,CAAC,GAAChC,CAAC,GAAC,CAAZ,EAAegC,CAAC,GAACkB,QAAQ,CAACjD,MAA1B,EAAkC+B,CAAC,EAAnC,EAAsC;AACrCY,UAAAA,MAAM,GAAG,KAAKO,cAAL,CAAoBH,uBAAuB,CAAC9B,GAAxB,CAA4BgC,QAAQ,CAAClD,CAAD,CAApC,CAApB,EAA8DgD,uBAAuB,CAAC9B,GAAxB,CAA4BgC,QAAQ,CAAClB,CAAD,CAApC,CAA9D,CAAT;;AACA,cAAGY,MAAM,GAAC,CAAV,EAAY;AACXK,YAAAA,KAAK,CAACG,OAAN,CAAcF,QAAQ,CAAClD,CAAD,CAAtB,EAA2BkD,QAAQ,CAAClB,CAAD,CAAnC,EAAwCY,MAAxC;AACA;AACD;AAED;;AACD,aAAOK,KAAP;AACA,K,CAED;;;;6BACSA,K,EAAM;AACd,UAAIC,QAAQ,GAAGD,KAAK,CAACI,cAAN,EAAf;AACA,UAAIC,aAAa,GAAG,IAAIxD,GAAJ,EAApB,CAFc,CAId;;AACA,UAAIoD,QAAQ,CAACjD,MAAT,IAAmB,CAAvB,EAAyB;AACxB,eAAOqD,aAAP;AACA;;AACD,UAAI9B,GAAG,GAAG0B,QAAQ,CAACK,IAAI,CAACC,KAAL,CAAWD,IAAI,CAACE,MAAL,KAAcP,QAAQ,CAACjD,MAAlC,CAAD,CAAlB;AACA,UAAIyD,MAAM,GAAGT,KAAK,CAACU,SAAN,CAAgBnC,GAAhB,CAAb;AACA,UAAIoC,gBAAgB,GAAG,EAAvB,CAVc,CAWd;;AACA,WAAK,IAAI5D,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG,KAApB,EAA2BA,CAAC,EAA5B,EAAgC;AAC/B,YAAI6D,WAAW,GAAG,CAAlB;AAEAH,QAAAA,MAAM,CAACI,QAAP,CAAgBxC,OAAhB,CAAwB,UAACC,KAAD,EAAQC,GAAR,EAAaC,GAAb,EAAmB;AAC1CoC,UAAAA,WAAW,IAAEtC,KAAb;AACA,SAFD;AAIAmC,QAAAA,MAAM,CAACI,QAAP,CAAgBxC,OAAhB,CAAwB,UAACC,KAAD,EAAQC,GAAR,EAAaC,GAAb,EAAmB;AAC1C,eAAI,IAAIsC,CAAC,GAAG,CAAZ,EAAeA,CAAC,GAACxC,KAAjB,EAAwBwC,CAAC,EAAzB,EAA4B;AAC3BH,YAAAA,gBAAgB,CAAC3B,IAAjB,CAAsBT,GAAtB;AACA;AACD,SAJD;AAOA,YAAIM,QAAQ,GAAG8B,gBAAgB,CAACL,IAAI,CAACC,KAAL,CAAWD,IAAI,CAACE,MAAL,KAAcG,gBAAgB,CAAC3D,MAA1C,CAAD,CAA/B;;AACA,YAAGqD,aAAa,CAACtC,GAAd,CAAkBc,QAAlB,CAAH,EAA+B;AAC9BwB,UAAAA,aAAa,CAACjD,GAAd,CAAkByB,QAAlB,EAA4BwB,aAAa,CAACpC,GAAd,CAAkBY,QAAlB,IAA4B,CAAxD;AACA,SAFD,MAEK;AACJwB,UAAAA,aAAa,CAACjD,GAAd,CAAkByB,QAAlB,EAA4B,CAA5B;AACA;;AACD,YAAIkC,WAAW,GAAGN,MAAlB;AACAA,QAAAA,MAAM,GAAGT,KAAK,CAACU,SAAN,CAAgB7B,QAAhB,CAAT;AACA8B,QAAAA,gBAAgB,GAAG,EAAnB;AACA;;AACD,aAAON,aAAP;AAEA;;;;;;AAMFW,MAAM,CAACC,OAAP,CAAe9E,YAAf,GAA8BA,YAA9B","sourcesContent":["const natural = require(\"natural\");\nconst WordPos = require(\"wordpos\");\nconst WeightedGraph = require('./WeightedGraph').WeightedGraph;\n\nclass Preprocesser{\n\tconstructor(){\n\t\tthis.tokenizer = new natural.SentenceTokenizer(); \n\t}\n\n\t//This method takes in a paragraph and returns a list of the sentences in the paragraph.\n\tparagraphToSentences(string_to_process){\n\t\ttry{\n\t\t\tlet result = this.tokenizer.tokenize(string_to_process);\n\t\t\treturn result;\n\t\t}catch(err){\n\t\t\treturn Error(\"Cannot toeknize the given string.\");\n\t\t}\n\t}\n\n\t//Cleans the sentences by removing punctuation and lowercasing capital letters.\n\tcleanSentences(list_to_clean){\n\t\tlet sentence_map = new Map();\n\t\tconst regex = /[&\\/\\\\#,+()$~%.'\":*?<>{}]/g;\n\t\tfor (let i = 0; i<list_to_clean.length; i++){\n\t\t\tlet original_sentence = list_to_clean[i];\n\t\t\tlist_to_clean[i] = list_to_clean[i].toLowerCase();\n\t\t\tlist_to_clean[i] = list_to_clean[i].replace(regex, \"\");\n\t\t\tsentence_map.set(list_to_clean[i], original_sentence);\n\t\t}\n\t\treturn [list_to_clean,sentence_map];\n\t}\n\n\t//Takes in a list of sentences and returns a list of all of the words in the sentences.\n\ttokenizeSentences(list_of_sentences){\n\t\tlet new_array = new Array();\n\t\tnew_array = list_of_sentences\n\t\tlet result_list = [];\n\t\tfor (let i = 0; i<new_array.length; i++){\n\t\t\tresult_list = result_list.concat(new_array[i].split(\" \"));\n\t\t}\n\t\treturn result_list;\n\t}\n\n\t//Takes in a list of words and calculates the frequencies of the words.\n\t//Returns a list. The first item is a map of word->frequency. The second is the max frequency.\n\tgetFrequencyAndMax(list_of_words){\n\t\tlet frequency_map = new Map();\n\t\tlet max = 0\n\t\tfor (let i = 0; i<list_of_words.length; i++){\n\t\t\tconst word = list_of_words[i];\n\t\t\tif (frequency_map.has(word)){\n\t\t\t\tconst new_val = frequency_map.get(word)+1;\n\t\t\t\tfrequency_map.set(word, new_val);\n\t\t\t\tif (new_val>max){\n\t\t\t\t\tmax = new_val;\n\t\t\t\t}\n\t\t\t}else{\n\t\t\t\tfrequency_map.set(word, 1);\n\t\t\t}\n\t\t}\n\t\treturn [frequency_map, max];\n\t}\n\t\n\t//Converts a frequency map into a map with \"weights\".\n\tgetWeights(list_of_words){\n\t\tconst frequencies_and_max = this.getFrequencyAndMax(list_of_words);\n\t\tconst frequencies_map = frequencies_and_max[0];\n\t\tconst max = frequencies_and_max[1];\n\t\tfrequencies_map.forEach((value,key,map)=>{\n\t\t\tmap.set(key, value/max);\n\t\t});\n\t\treturn frequencies_map;\n\t}\n\n\n\tsentenceWeights(clean_sentences, weighted_map){\n\t\tlet weight_of_sentence = 0;\n\t\tlet sentence_weight_list = [];\n\t\tlet sentence = \"\";\n\t\tfor (let i = 0; i<clean_sentences.length; i++){\n\t\t\tsentence = clean_sentences[i];\n\t\t\tlet word_list = sentence.split(\" \");\n\t\t\tweight_of_sentence = 0;\n\t\t\tfor (let j = 0; j<word_list.length; j++){\n\t\t\t\tweight_of_sentence += weighted_map.get(word_list[j]);\n\t\t\t}\n\t\t\tsentence_weight_list.push([weight_of_sentence/word_list.length, sentence]);\n\t\t}\n\t\treturn sentence_weight_list;\n\t}\n\n\t//Takes a list of sentences and returns a map of the each sentence to its nouns and adjectives\n\tasync nounsAndAdjectives(clean_sentences){\n\t\tlet nouns_and_adjectives_map = new Map();\n\t\tlet wordpos = new WordPos();\n\t\ttry{\n\t\t\tfor (let i = 0; i<clean_sentences.length; i++){\n\t\t\t\tlet adjectives = await wordpos.getAdjectives(clean_sentences[i]);\n\t\t\t\tlet nouns = await wordpos.getNouns(clean_sentences[i]);\n\t\t\t\tnouns_and_adjectives_map.set(clean_sentences[i],nouns.concat(adjectives));\n\t\t\t}\n\n\t\t\treturn await nouns_and_adjectives_map;\n\t\t}catch(err){\n\t\t\tconsole.log(err)\n\t\t\treturn\n\t\t}\n\t}\n\n\t//Used for the text rank summary. Takes two lists of words and gets the weight of the edge connecting the vertices.\n\tgetEdgeWeights(list1, list2){\n\t\tlet weight = 0;\n\t\tlet intial = list1\n\t\tlet other = list2\n\t\tif (list2.length >= list1.length){\n\t\t\tintial = list2\n\t\t\tother = list1\n\t\t}\n\t\tfor(let i=0; i<intial.length; i++){\n\t\t\tif(other.includes(intial[i])){\n\t\t\t\tweight+=1;\n\t\t\t}\n\t\t}\n\n\t\treturn weight\n\t}\n\n\t//Creates the graph for the textrank algorithm.\n\tcreateTextRankGraph(nouns_and_adjactive_map){\n\t\tlet graph = new WeightedGraph();\n\t\tlet key_list = [];\n\t\tlet weight = 0\n\t\tnouns_and_adjactive_map.forEach((value,key,map)=>{\n\t\t\tkey_list.push(key);\n\t\t})\n\t\tfor(let i=0; i<key_list.length; i++){\n\t\t\tfor(let j=i+1; j<key_list.length; j++){\n\t\t\t\tweight = this.getEdgeWeights(nouns_and_adjactive_map.get(key_list[i]), nouns_and_adjactive_map.get(key_list[j]));\n\t\t\t\tif(weight>0){\n\t\t\t\t\tgraph.addEdge(key_list[i], key_list[j], weight);\n\t\t\t\t}\n\t\t\t}\n\n\t\t}\n\t\treturn graph;\n\t}\n\n\t//TextRank algorithm.\n\ttextRank(graph){\n\t\tlet key_list = graph.getAllVertices();\n\t\tlet text_rank_map = new Map();\n\t\t\n\t\t//random key to start with\n\t\tif (key_list.length == 0){\n\t\t\treturn text_rank_map;\n\t\t}\n\t\tlet key = key_list[Math.floor(Math.random()*key_list.length)];\n\t\tlet vertex = graph.getVertex(key);\n\t\tlet probability_list = [];\n\t\t//random walk \n\t\tfor (let i = 0; i < 10000; i++) {\n\t\t\tlet full_weight = 0\n\t\t\n\t\t\tvertex.adjacent.forEach((value, key, map)=>{\n\t\t\t\tfull_weight+=value;\n\t\t\t})\n\t\t\n\t\t\tvertex.adjacent.forEach((value, key, map)=>{\n\t\t\t\tfor(let x = 0; x<value; x++){\n\t\t\t\t\tprobability_list.push(key);\n\t\t\t\t}\n\t\t\t})\n\t\t\n\n\t\t\tlet sentence = probability_list[Math.floor(Math.random()*probability_list.length)];\n\t\t\tif(text_rank_map.has(sentence)){\n\t\t\t\ttext_rank_map.set(sentence, text_rank_map.get(sentence)+1)\n\t\t\t}else{\n\t\t\t\ttext_rank_map.set(sentence, 1);\n\t\t\t}\n\t\t\tlet last_vertex = vertex;\n\t\t\tvertex = graph.getVertex(sentence);\n\t\t\tprobability_list = [];\n\t\t}\n\t\treturn text_rank_map;\n\t\t\n\t}\n\n\n}\n\n\nmodule.exports.Preprocesser = Preprocesser"]},"metadata":{},"sourceType":"script"}